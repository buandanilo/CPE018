{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d44fc7c-ab9b-43cc-919b-020cf9fce309",
   "metadata": {},
   "source": [
    "# CPE018 Midterm Exam (1st Sem, A.Y. 2023-2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c73f82-2df6-45d1-852f-70f4f7fe0b73",
   "metadata": {},
   "source": [
    "Student Submission Details:\n",
    "* Name: Danilo Jr. G. Buan\n",
    "* Section: CPE31S3\n",
    "* Schedule: Saturday : 10:30 - 1:30\n",
    "* Instructor:Engr. Roman M. Richard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fed455-ea33-4e13-a725-0cd25e99041b",
   "metadata": {},
   "source": [
    "## Intended Learning Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190f5a5b-4ac1-40f8-9b24-d244e929a3d3",
   "metadata": {},
   "source": [
    "By the end of this activity, the student should be able to:\n",
    "* ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "* ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9993c5-0f71-4bdc-8f4b-af50975b9d91",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8324f2-2f10-4a8b-873f-400c32338886",
   "metadata": {},
   "source": [
    "For this examination, you must create a **mood detection** program with an object-oriented programming approach (same as project CAMEO), it must detect mood changes through the use of algorithms/techniques/schemes learned in class, and from external sources.\n",
    "\n",
    "In this file, you have to include for each section of your solution your completion of the following:\n",
    "\n",
    "* Part 1: **Face Detection**: Once your face is detected using any algorithm, it must draw an ROI. The color for the ROI is your choice; however, it must detect for all faces in the frame and draw a corresponding ROI.\n",
    "* Part 2: **Face Recognition**: The detected face must then be recognized, using any of the provided tools in class, the ROIs must indicate whether it is your face or someone it doesn't recognize.\n",
    "* Part 3: **Mood Detection**: Use three different feature detection and matching techniques to determine three emotion: happy, sad and neutral. Two of the techniques must be learned from class, and 1 must be one you independently learned.\n",
    "\n",
    "Properly show through your notebook the output for each part of the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b3b1fb-327f-4723-aea3-543f1221c683",
   "metadata": {},
   "source": [
    "## Procedure and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef60d7f-d688-4ead-b9db-36b3739cc3ff",
   "metadata": {},
   "source": [
    "Notes:\n",
    "* This is the section where you have to include all  your answers to the items provided in the tasks section.\n",
    "* Tasks 1 and 2 contribute directly to ILO1: Demonstrate different methods for feature matching and detection learned in class and indepdentently from new sources.\n",
    "* Task 3 contributes directly to ILO2: Evaluate the accuracy of different feature matching and detection methods and scrutinize its applicability in solving a given real-life problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde1091-4711-4343-a48d-edad768437dd",
   "metadata": {},
   "source": [
    "### Task 1: Face Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13054d3d-fc5e-4e43-bdac-2f699165dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect():\n",
    "    face_cascade_path = r'F:\\Desktop\\MIDTERMS EXAMS\\haarcascade_frontalface_default.xml'\n",
    "    face_cascade = cv2.CascadeClassifier(face_cascade_path)\n",
    "\n",
    "    camera = cv2.VideoCapture(0)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = camera.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            img = cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            roi_color = frame[y:y + h, x:x + w]\n",
    "\n",
    "        cv2.imshow(\"camera\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\" \"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2e01e",
   "metadata": {},
   "source": [
    "# Output for Face Detection\n",
    "![](facedetect.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd46e7b-4155-47de-9ada-a775afb39b2d",
   "metadata": {},
   "source": [
    "### Task 2: Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71ae16-2dd0-4171-8a69-4129338d695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import errno\n",
    "import sys\n",
    "import cv2\n",
    "\n",
    "def read_images(path, sz=None):\n",
    "    c = 0\n",
    "    X, y = [], []\n",
    "\n",
    "    for dirname, dirnames, filenames in os.walk(path):\n",
    "        for subdirname in dirnames:\n",
    "            subject_path = os.path.join(dirname, subdirname)\n",
    "            for filename in os.listdir(subject_path):\n",
    "                try:\n",
    "                    if filename == \".directory\":\n",
    "                        continue\n",
    "                    filepath = os.path.join(subject_path, filename)\n",
    "                    im = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                    # Resize the images to the prescribed size if sz is provided\n",
    "                    if sz is not None:\n",
    "                        im = cv2.resize(im, sz)\n",
    "\n",
    "                    X.append(np.asarray(im, dtype=np.uint8))\n",
    "                    y.append(c)\n",
    "\n",
    "                except IOError as e:\n",
    "                    print(f\"I/O Error({e.errno}): {e.strerror}\")\n",
    "                except Exception as e:\n",
    "                    print(\"Unexpected error:\", e)\n",
    "\n",
    "            c += 1\n",
    "\n",
    "    return [X, y]\n",
    "\n",
    "def face_rec():\n",
    "    names = ['BUAN','UNKNOWN']  # Put your names here for faces to recognize\n",
    "\n",
    "    # Provide the path to your image dataset folder here\n",
    "    dataset_path = 'F:\\Desktop\\MIDTERMS EXAMS\\DATASET'\n",
    "\n",
    "    # Call the function to read images from the dataset directory\n",
    "    X, y = read_images(dataset_path)\n",
    "    y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "    # Create a face recognition model\n",
    "    model = cv2.face.EigenFaceRecognizer_create()\n",
    "    model.train(X, y)\n",
    "\n",
    "    camera = cv2.VideoCapture(0)\n",
    "    face_cascade = cv2.CascadeClassifier('F:\\Desktop\\MIDTERMS EXAMS\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "    while True:\n",
    "        ret, img = camera.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        faces = face_cascade.detectMultiScale(img, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            gray = cv2.cvtColor(img[y:y + h, x:x + w], cv2.COLOR_BGR2GRAY)\n",
    "            roi = cv2.resize(gray, (200, 200), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "            try:\n",
    "                params = model.predict(roi)\n",
    "                label = names[params[0]]\n",
    "                cv2.putText(img, label, (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "            except:\n",
    "                cv2.putText(img, \"UNKNOWN\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"camera\", img)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    camera.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    face_rec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0ae406",
   "metadata": {},
   "source": [
    "# Output for Face Recognition\n",
    "![](facerecog.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5b345-0442-4e42-8500-a247a520383e",
   "metadata": {},
   "source": [
    "### Task 3: Mood Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc361ca0-d355-4470-9fba-2b3b67bb240b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "\n",
    "emotion_dict = {0: \"Happy\", 1: \"Neutral\", 2: \"Sad\"}\n",
    "\n",
    "\n",
    "json_file = open('model/emotion_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "emotion_model = model_from_json(loaded_model_json)\n",
    "\n",
    "\n",
    "emotion_model.load_weights(\"model/emotion_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    frame = cv2.resize(frame, (1280, 720))\n",
    "    if not ret:\n",
    "        break\n",
    "    face_detector = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "    num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "\n",
    "    for (x, y, w, h) in num_faces:\n",
    "        cv2.rectangle(frame, (x, y-50), (x+w, y+h+10), (0, 255, 0), 4)\n",
    "        roi_gray_frame = gray_frame[y:y + h, x:x + w]\n",
    "        cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame, (48, 48)), -1), 0)\n",
    "\n",
    "\n",
    "        emotion_prediction = emotion_model.predict(cropped_img)\n",
    "        maxindex = int(np.argmax(emotion_prediction))\n",
    "\n",
    "\n",
    "        if maxindex in emotion_dict:\n",
    "            cv2.putText(frame, emotion_dict[maxindex], (x+5, y-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad50cea",
   "metadata": {},
   "source": [
    "# Output For Mood Detection\n",
    "## Happy\n",
    "![](mood_happy.jpg)\n",
    "## Neutral\n",
    "![](mood_neutral.jpg)\n",
    "## Sad\n",
    "![](mood_sad.jpg)\n",
    "![](try_mood.jpg)\n",
    "![](try2_mood.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8c97cb-2d49-4de4-9067-04245f9ab9cf",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c6947-908c-4dd2-89c7-7dc4bb2f58f8",
   "metadata": {},
   "source": [
    "For the three different techniques you used in face detection, provide an in-depth analysis. \n",
    "\n",
    "To do this, you must:\n",
    "* Test the face detection, face recongition, and mood detection functions 10 times each. Only the mood detection will have components for 10 tests for each different technique used.\n",
    "* Create a table containing the 10 tests (like shown below) for each task.\n",
    "* Analyze each output by identifying the accuracy and providing your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b079aab-6780-455a-9b69-b779028a02a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 3A: Mood Detection using XYZ Algorithm\n",
      "+----------+------------+----------+---------+\n",
      "|   Test # | Expected   | Actual   |   Score |\n",
      "+==========+============+==========+=========+\n",
      "|        1 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        2 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        3 | Happy      | Happy    |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        4 | Neutral    | Neutral  |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        5 | Neutral    | Neutral  |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        6 | Neutral    | Neutral  |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        7 | Neutral    | Neutral  |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        8 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|        9 | Sad        | Sad      |       1 |\n",
      "+----------+------------+----------+---------+\n",
      "|       10 | Sad        | Neutral  |       0 |\n",
      "+----------+------------+----------+---------+\n",
      "Accuracy:  90.0\n"
     ]
    }
   ],
   "source": [
    "# Import the module for tabulating the data\n",
    "from tabulate import tabulate\n",
    " \n",
    "# Create a list for content of the table\n",
    "test_results = [\n",
    "    [\"1\", \"Happy\", \"Happy\", 1],\n",
    "    [\"2\", \"Happy\", \"Happy\", 1],\n",
    "    [\"3\", \"Happy\", \"Happy\", 1],\n",
    "    [\"4\", \"Neutral\", \"Neutral\", 1],\n",
    "    [\"5\", \"Neutral\", \"Neutral\", 1],\n",
    "    [\"6\", \"Neutral\", \"Neutral\", 1],\n",
    "    [\"7\", \"Neutral\", \"Neutral\", 1],\n",
    "    [\"8\", \"Sad\", \"Sad\", 1],\n",
    "    [\"9\", \"Sad\", \"Sad\", 1],\n",
    "    [\"10\", \"Sad\", \"Neutral\", 0]\n",
    "]\n",
    " \n",
    "# Create a list for the headers of your table\n",
    "header = [\"Test #\", \"Expected\", \"Actual\", \"Score\"]\n",
    " \n",
    "# display table\n",
    "print(\"Task 3A: Mood Detection using XYZ Algorithm\")\n",
    "print(tabulate(test_results, headers=header, tablefmt=\"grid\"))\n",
    "\n",
    "# Calculate for the accuracy\n",
    "total = 0\n",
    "for i in test_results:\n",
    "    total += i[3]\n",
    "print(\"Accuracy: \", round(total/len(test_results)*100,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfcc148-9c95-4125-a25c-965f16a7260e",
   "metadata": {},
   "source": [
    "## Summary and Lessons Learned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234d297-014b-4ab6-a427-627cf25b7632",
   "metadata": {},
   "source": [
    "         For this midterm exam, my skills were put to the test in creating a mood detection program. At first, my knowledge on face detection and recognition were jogged up and refreshed that helped me in the last task that was to implement a mood detection program. Since I myself is not very good at coding, watching youtube videos and researching about mood detection really helped me implement this successfully. After performing this activity, this helped me refreshen my knowledge about face detection and face recognition and also the addition of mood detection. Mood detection is somewhat similar to face recognition since training models were involved. The dataset that I have used for the three moods (happy, neutral, and sad) were available on the internet so it was a little bit relieving. The use of other modules such as keras that is used for deep learning really helped a lot for successfully implementing the last task. As seen on the table above, the accuracy is 90% which is very high. Also, I tried to use pictures of other people to test if it can detect their moods also."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb275b8-d25b-4b4e-b1f3-e7ae48fd9de5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c9194e-b263-4d38-bfa3-ed6490825a43",
   "metadata": {},
   "source": [
    "**Proprietary Clause**\n",
    "\n",
    "Property of the Technological Institute of the Philippines (T.I.P.). No part of the materials made and uploaded in this learning management system by T.I.P. may be copied, photographed, printed, reproduced, shared, transmitted, translated or reduced to any electronic medium or machine-readable form, in whole or in part, without prior consent of T.I.P.\n",
    "\n",
    "Prepared by Engr. RMR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
